{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    font-weight: bold;\n",
       "    src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    font-style: oblique;\n",
       "    src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    font-weight: bold;\n",
       "    font-style: oblique;\n",
       "    src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}\n",
       "div.cell{\n",
       "    width:1000px;\n",
       "    margin-left:4% !important;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "/* Change global font size (for code) */\n",
       ".CodeMirror {\n",
       "font-size: 11pt !important;\n",
       "/*font-family: monospace;*/\n",
       "}\n",
       "h1 {\n",
       "    font-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "    margin-top:12px;\n",
       "    margin-bottom: 3px;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: \"Computer Modern\", \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "    line-height: 160%;\n",
       "    font-size: 115%;\n",
       "    width:850px;\n",
       "    margin-left:auto;\n",
       "    margin-right:900px;\n",
       "}\n",
       ".CodeMirror{\n",
       "    font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       "/*.prompt{\n",
       "    display: None;\n",
       "}*/\n",
       ".text_cell_render h5 {\n",
       "    font-weight: 300;\n",
       "    font-size: 22pt;\n",
       "    color: #4057A1;\n",
       "    font-style: italic;\n",
       "    margin-bottom: .5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "    color: rgb( 240, 20, 20 )\n",
       "}\n",
       "/* Highlight boxes */\n",
       ".jp-RenderedHTMLCommon .alert-info {\n",
       "    color: var(--jp-info-color0);\n",
       "    color: #004E57; /* better contrast */\n",
       "    background-color: var(--jp-info-color3);\n",
       "    border-color: var(--jp-info-color2);\n",
       "}\n",
       "/* Footnotes style */\n",
       ".footnote {\n",
       "    font-size: 80%;\n",
       "}\n",
       "/*\n",
       " * Markdown autonumber headers (see https://stackoverflow.com/questions/19999696/are-numbered-headings-in-markdown-rdiscount-possible)\n",
       " */\n",
       "body {\n",
       "    counter-reset: h1 h2 h3 h4;\n",
       "    /* h2 NEEDED! this makes sure the scope of h2 is kind of global */\n",
       "}\n",
       "\n",
       "h1 {\n",
       "    counter-increment: h1;\n",
       "    counter-set: h2;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    counter-increment: h2;\n",
       "    counter-set: h3; /*I've got no idea why this works and counter-reset not...*/\n",
       "}\n",
       "\n",
       "h3 {\n",
       "    counter-increment: h3;\n",
       "    counter-set: h4;\n",
       "}\n",
       "\n",
       "h2:before {\n",
       "    content: counter(h2) \" \";\n",
       "}\n",
       "\n",
       "h3:before {\n",
       "    content: counter(h2) \".\" counter(h3) \". \"\n",
       "}\n",
       "\n",
       "h4:before {\n",
       "    content: counter(h2) \".\" counter(h3) \".\" counter(h4) \". \"\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "Use %matplotlib inline instead of %matplotlib widget when preparing the final report. \n",
    "Otherwise the images are, unfortunately, not embedded in the pdf. \n",
    "'''\n",
    "from importstatements import *\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\unit}[1]{\\ensuremath{\\text{#1}}}\n",
    "\\newcommand{\\tmidx}{m}\n",
    "\\newcommand{\\matcmd}[2][(\\cdot)]{\\texttt{#2}#1}\n",
    "\\renewcommand{\\matcmd}[2][(\\cdot)]{\\mathrm{#2}#1}\n",
    "\\newcommand{\\fcn}[1]{{\\text{#1}}}\n",
    "\\newcommand{\\bigcb}[1]{{\\big\\{#1\\big\\}}} \n",
    "\\newcommand{\\Bigcb}[1]{{\\Big\\{#1\\Big\\}}}\n",
    "\\newcommand{\\bigsb}[1]{{\\big[#1\\big]}} \n",
    "\\newcommand{\\Bigsb}[1]{{\\Big[#1\\Big]}}\n",
    "\\newcommand{\\biggsb}[1]{{\\bigg[#1\\bigg]}}\n",
    "\\newcommand{\\parvec}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\featvec}[1][m]{\\mathbf{x}^{(#1)}}\n",
    "\\newcommand{\\featmat}{\\mathbf{X}}\n",
    "\\newcommand{\\featlbl}[1][m]{y^{(#1)}}\n",
    "\\newcommand{\\estfeatlbl}[1][m]{\\widehat{y}^{(#1)}}\n",
    "\\newcommand{\\lblvec}{\\mathbf{y}}\n",
    "\\newcommand{\\estlblvec}{\\widehat{\\mathbf{y}}}\n",
    "$\n",
    "\n",
    "# Logistic regression\n",
    "In this exercise, we will get hands-on experience with _supervised classification_ of data. We will look at the simple _binary classification_ problem, where the data must be categorised as belonging to 'Class 0' or 'Class 1'. In this exercise, you will not only learn to implement the approaches but also learn the standard good-practices that are applied in data-analysis.\n",
    "\n",
    "We will first define help functions that allow us to predict the outcome given the model parameters ($\\parvec$), calculate the log-likelihood and the accuracy of the designed classifier. \n",
    "\n",
    "Then, we will implement the logistic regression algorithm for estimating $\\parvec$ and test it on a toy (synthetic)-dataset. Once we are satisfied by the performance on the toy-data, we will test the approach on a _real-world_ dataset in which we would like to be able to predict whether a patient has diabetes or not, given clinical information relating to the patient. \n",
    "\n",
    "## Definition of helper functions\n",
    "In this section we shall define the basic functions we shall need to implement the logistic regression based classifier.\n",
    "\n",
    "### Definition of the `sigmoid` function\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Define the function `Sigmoid` that returns the sigmoid of the input `x` as:\n",
    "$$\\text{sigmoid}(x) = \\dfrac{1}{1 + \\exp{(-x)}}$$    \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the sigmoid function. Enter your code here\n",
    "def Sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the log-likelihood function\n",
    "\n",
    "As we have seen in the lecture, the parameters $\\parvec$ are obtained by running several iterations on the training data. During this training, we could use the log-likelihood of the data in the training set to see if the training is proceeding in the correct direction (log-likelihood keeps increasing) and how close we are to convergence (to break the iterations when the algorithm/training is converged).\n",
    "\n",
    "Consider that we have a training data set consisting of $M$ _augmented_ feature vectors $\\featvec$ (i.e., feature vectors that have been extended with a 1 as the first element (see last lecture slide)). Let $N$ be the dimension of this augmented feature vector (i.e., $\\featvec\\in\\mathbb{R}^{(N\\times 1)}$). We can collect these vectors into an $(N\\times M)$ matrix $\\featmat = \\Big(\\featvec[0],\\featvec[1],\\ldots,\\featvec[M-1] \\Big)$. Similarly, we can collect the $M$ class labels $\\featlbl$ into a $(M\\times 1)$ _column_ vector as $\\lblvec = \\big(\\featlbl[0], \\featlbl[1],\\ldots ,\\featlbl[M-1]\\big)^T$\n",
    "\n",
    "At any iteration $k$, given the estimate of the classifier parameters ${\\parvec}_{(k)}$, the 'goodness' of that parameter estimate is reflected in the log-likelihood value. Recollect that the log-likelihood was defined as:\n",
    "$$\\log\\,\\mathcal{L}\\big(\\mathbf{y}\\rvert\\mathbf{X};{\\parvec}_{(k)}\\big) = \\sum_{m}\\log\\,\\mathcal{L}\\big(y^{(m)}\\rvert\\mathbf{x}^{(m)};{\\parvec}_{(k)}\\big)\\,,$$\n",
    "\n",
    "Further, recall that $\\mathcal{L}(y\\rvert\\mathbf{x};\\parvec) = h_{\\parvec}^{y}(\\mathbf{x})\\big(1-h_{\\parvec}(\\mathbf{x})\\big)^{(1-y)}$ \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement a function that returns the log-likelihood for a given set of feature vectors $\\featmat$, the corresponding set of class labels $\\lblvec$, and the logistic regression parameters $\\parvec$. \n",
    "    \n",
    "Use the template: `logLikelihood = LogLikelihood(X,y,theta)`\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-likelihood function here.\n",
    "\n",
    "def LogLikelihood(X,y,theta):\n",
    "    logLikelihood = np.sum(y*np.log(Sigmoid(theta.T@X))+(1-y)*np.log(1-Sigmoid(theta.T@X)))\n",
    "    return logLikelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the label of a test-data point\n",
    "Now, assuming we have an estimate the sigmoid paramter ${\\parvec}$, we shall write a function to compute the label prediction $\\widehat{y}$ for a given test data-point $\\mathbf{x}$. This function can be used in several ways:\n",
    "1. in determining the log-likelihood during the training phase\n",
    "1. in computing the performance of the classifier on the test data \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "To implement this function, take the following steps:\n",
    "    \n",
    "* Compute $h_{\\parvec}(\\mathbf{x})$. We know that $0\\leq h_{\\parvec}(\\mathbf{x}) \\leq 1$. \n",
    "* However, the logistic regression classifier should output a class _label_ $\\widehat{y} \\in\\{0, 1\\}$. \n",
    "* Thus, to map from the continuous value $h_{\\parvec}(\\mathbf{x})$ to a binary label, we need to quantise $h_{\\parvec}(\\mathbf{x})$. We do this by setting a threshold $\\Gamma$ on $h_{\\parvec}(\\mathbf{x})$ such that:\n",
    "$$ \\widehat{y} = \\begin{cases} 1 & h_{\\parvec}(\\mathbf{x}) \\geq \\Gamma \\\\\n",
    "                               0 & \\text{otherwise} \\end{cases}$$\n",
    "* This will classify the data point into `Class 1` or `Class 0`. We typically choose $\\Gamma = 0.5$.   \n",
    "\n",
    "Use the template: `yHat = PredictLabel(x,theta)` for this function. \n",
    "    \n",
    "Note: if you are feeling efficient, you can modify this function so that it returns the label for a whole _set_ of input data points in the form of the data matrix $\\mathbf{X}$.\n",
    "\n",
    "**Tip:** Since we will be using the bias term in our definiton of the logistic function, take this into account when you input the feature vectors and the parameter vectors (i.e., if your input is the original ( _unaugmented_ ) feature, then remember to attach the row of `1` to the matrix (see last slide of lecture). \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the label of data, given the features.\n",
    "def PredictLabel(X,theta):\n",
    "    # Add your code here: first compute h_theta \n",
    "    h_theta = Sigmoid(theta.T@X)\n",
    "    # Next, quantise the value to the two class labels here. \n",
    "    # gamma = 0.5 is the threshold for the quantisation.\n",
    "    yHat = (h_theta>=0.5)*1\n",
    "    return yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the accuracy of the classifier\n",
    "\n",
    "The metrics that define the accuracy of a classifier are computed on the basis of a so-called _confusion matrix_. This is obtained by comparing the _predicted_ labels $\\estfeatlbl$ to the _true_ label $\\featlbl$. Of course, we can only compute the confusion matrix when the _true_ labels are available... which does not occur very often in practice. What is typically done is to compute this on the training data, to judge the accuracy of the trained classifier. This method is often adapted when comparing two different classifiers. \n",
    "\n",
    "Assume we have $M$ data-points[<sup>1</sup>](#fn1) for which the true labels $\\featlbl$ are available and the predicted labels $\\estfeatlbl$ have been computed. For the binary classifier we have seen in the lecture, the confusion matrix $\\boldsymbol{\\mathcal{C}}$ is typically written as a $(2\\times 2)$ matrix where the elements $\\mathcal{C}(i,j)$ indicate the _number_ of times $\\estfeatlbl = i$ and $\\featlbl=j$ where $i,j \\in \\bigcb{0,1}$\n",
    "\n",
    "<span id=\"fn1\"> <sup>1</sup> The terms data-points and feature vectors are used interchangeably </span>\n",
    "\n",
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Write such a function `GetConfusionMatrix` to compute the confusion matrix for a given trained classifier. \n",
    "    \n",
    "Use the template `confMatrix = GetConfusionMatrix(y,yHat)`, where `y` is the set of correct class labels and `yHat` is the set of predicted data labels. \n",
    "    \n",
    "</div>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix of the classifier\n",
    "def GetConfusionMatrix(yTrue,yHat):\n",
    "    C = np.zeros((2,2))\n",
    "    C[0,0] = np.sum((yTrue == 0)*(yHat == 0)*1)\n",
    "    C[0,1] = np.sum((yTrue == 1)*(yHat == 0)*1)\n",
    "    C[1,0] = np.sum((yTrue == 0)*(yHat == 1)*1)\n",
    "    C[1,1] = np.sum((yTrue == 1)*(yHat == 1)*1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the accuracy of the classifier\n",
    "Based on the confusion matrix, we shall now compute the accuracy of the classifier in terms of the true negatives (TN), true positives (TP), false negatives (FN) and false positives (FP). Typically, these are expressed as a _percentage_ of the total number of data-points within a class. Thus, for example, the TN _rate_ or TN _percentage_ indicates, of all the data points with true label $0$, what percentage of this is correctly predicted by the classifier:\n",
    "$$\\text{TN}(\\%) = 100\\, \\dfrac{\\fcn{TN}}{\\fcn{TN}+\\fcn{FP}}$$ \n",
    "\n",
    "Similarly, we may define $\\fcn{TP}(\\%)$ and so on.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write down the expression for $\\fcn{TP}(\\%)$, $\\fcn{FN}(\\%)$, $\\fcn{FP}(\\%)$ using the convention described above.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<span style=\"color:black\">\n",
    "  \n",
    "$$\\fcn{TP}(\\%) =  100\\, \\dfrac{\\fcn{TP}}{\\fcn{FN}+\\fcn{TP}} $$\n",
    "    \n",
    "$$\\fcn{FN}(\\%) =  100\\, \\dfrac{\\fcn{FN}}{\\fcn{FN}+\\fcn{TP}} $$\n",
    "    \n",
    "$$\\fcn{FP}(\\%) =  100\\, \\dfrac{\\fcn{FP}}{\\fcn{TN}+\\fcn{FP}} $$\n",
    "    \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "Write a function `ComputeMetrics` that takes the confusion matrix as an input and computes the performance metrics for this classifier, as we have seen in the lecture. \n",
    "    \n",
    "The result should be expressed in percentage, as described above.\n",
    "    \n",
    "Check your result using the confusion matrix $\\boldsymbol{\\mathcal{C}} = \\begin{pmatrix}90 & 10\\\\20 & 130 \\end{pmatrix}$. \n",
    "    \n",
    "**Tip:** Expected values: $\\fcn{TP}(\\%) = 92.86$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the performance metrics for binary classification\n",
    "def ComputeMetrics(C):\n",
    "\n",
    "    # Calculate the rate/percentage of true positives, false positives, false negatives, true negatives.\n",
    "    TPrate = 100*C[1,1]/(C[1,1]+C[0,1])\n",
    "    FPrate = 100*C[1,0]/(C[0,0]+C[1,0])\n",
    "    FNrate = 100*C[0,1]/(C[0,1]+C[1,1])\n",
    "    TNrate = 100*C[0,0]/(C[0,0]+C[1,0])\n",
    "    # print the result: \n",
    "    print(\"TP(%%): %3.2f, FP(%%): %3.2f, FN(%%): %3.2f, TN(%%): %3.2f\"%(TPrate,FPrate,FNrate,TNrate))\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP(%): 92.86, FP(%): 18.18, FN(%): 7.14, TN(%): 81.82\n"
     ]
    }
   ],
   "source": [
    "# Define the confusion matrix as described in the task\n",
    "# Compute the metrics on this confusion matrix.\n",
    "C = np.array([[90,10],[20,130]])\n",
    "ComputeMetrics(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the logistic regression classifier\n",
    "\n",
    "We are finally in a position to implement a logistic regression classifier. We shall follow the Newton method, since this converges much faster. This function is given to you below.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Go through the function and fill in the missing elements ($<??>$).\n",
    "* Decide and implement an appropriate stopping criterion for the iterations (**Tip:** The log-likelihood function is useful for this)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression parameter finding using the Hessian for the update rule.\n",
    "\n",
    "def LogisticRegressionNewton(X,y,augmentFeatures=False,dbgFlag=True):\n",
    "    \n",
    "    nDim,nSamp = X.shape\n",
    "    \n",
    "    # If not already done, augment the data matrix using the following code\n",
    "    if augmentFeatures==True:\n",
    "        X = np.append(np.ones((1,nSamp)),X,axis=0)\n",
    "        nDim += 1\n",
    "    \n",
    "    thetaVec = np.zeros(nDim) # Vector of parameters\n",
    "    \n",
    "    # Now iterate over the data several times until you are confident the approach\n",
    "    # has converged. Usually, the loglikelihood function can show you when the parameters\n",
    "    # have converged (the log-likelihood should increase and then become almost constant)\n",
    "    iterateFlag = 1\n",
    "    iterC = 0\n",
    "    lll = np.inf\n",
    "    while (iterateFlag > 0):\n",
    "        iterC += 1\n",
    "        yHat = Sigmoid(thetaVec@X) # Compute the output of the Sigmoid. DO NOT QUANTISE this.\n",
    "        \n",
    "        grad = ((y-yHat)*X).sum(axis=1)  # Compute the gradient (over all the samples)\n",
    "        hess = (yHat*(1-yHat)*X).dot(X.T) # Compute the Hessian matrix\n",
    "        \n",
    "        # Regularise the Hessian to prevent a badly conditioned matrix from\n",
    "        # blowing-up your solution.\n",
    "        hess += 1e-3*np.identity(nDim)\n",
    "\n",
    "        thetaVec = thetaVec + (np.linalg.inv(hess)).dot(grad) # Compute the Newton's update\n",
    "\n",
    "        # Implement the stopping criterion/stopping logic\n",
    "        logLikelihood = LogLikelihood(X,y,thetaVec)\n",
    "        # Implement the appropriate criterion\n",
    "\n",
    "        if np.isnan(logLikelihood):\n",
    "            stop_criterion = True\n",
    "            logLikelihood = lll\n",
    "        else:\n",
    "            stop_criterion = (np.abs(logLikelihood-lll)<0.1) + (np.abs(logLikelihood)<0.01)\n",
    "            lll = logLikelihood\n",
    "        \n",
    "        if (stop_criterion): \n",
    "            iterateFlag = 0\n",
    "        \n",
    "        if dbgFlag==True:\n",
    "            # Print some debug information            \n",
    "            print('Iteration %d. Log-likelihood: %3.3f'%(iterC,logLikelihood))\n",
    "            \n",
    "            \n",
    "    return thetaVec\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier\n",
    "\n",
    "### Simple 2-dimensional data\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "* Use the following code to generate $K=150$ samples of synthetic two-dimensional data. This code generates two data-sets: a training data-set of 100 samples and a test-data set of 50 samples. \n",
    "* Train your classifier on the training data, see how the log-likelihood evolves. Stop the iterations when you feel the training has converged.\n",
    "* Next, test the trained classifiers on the test data-set and report your results in terms of the confusion matrix and the performance metrics. \n",
    "* Also, try to visualise your classifier results on a two-dimensional plane using `scatter`. See an example in the code below. Also refer to the `Matplotlib` documentation for more information.\n",
    "    \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5iU1fXA8e/ZZQvsLnVpy1KlNxWQooJYUIqKRI0giaIiRkR/MdFoLDFqEo2JxBIV0SAYRYIIiogNRbGASO+9Lktv29g69/fHna0zuzvstN2Z83meeZj3zjvzntnRM+/c995zxRiDUkqp0BcR7ACUUkoFhiZ8pZQKE5rwlVIqTGjCV0qpMKEJXymlwoQmfKWUChOa8JU6CyJiRKR9AI4jIvKWiJwUkeUe7N/GGVstf8emai5N+MrnRORiEflRRE6LyAkR+UFELvDyNceJyPdl2qaLyF+8i9Y/3MV7li4GhgDJxpi+PgoLABEZLCIpvnxNVTPo2YDyKRGpCywA7gZmA9HAQCAnmHG5IyK1jDH5wY6jHK2BPcaYzGAHokKIMUZvevPZDegDnKpknzuBzUA6sAno5Wx/GNhZon2Us70LkA0UABnAKWACkAfkOts+du6bBHwAHAV2A/eVOO6fgTnAO0AaMN5NbNOBKcCXzji+BVqXeNwA7Z336wFvO4+1F3gM+6vZJd5y/g5JwHzgBLADuNPZfkeZ5z/p5rmRwD+BY8Au4B5nbLWcj99W4m+8C7jL2R4HnAEcztfOcMbRF1jq/NseBP4NRAf7vye9+fYW9AD0Flo3oC5wHJgBDAMalHn8RuAAcAEgQPvChOp8LMmZNG8CMoHmzsfGAd+Xea3pwF9KbEcAK4E/YX9ZtHMmu6ucj//Z+SVxnXPf2m7in+5MkoOAGODFksctk/DfBj4CEoA2wDbgjvLidXOsb4FXgVjgPOcXx+WePB/4DbAFaAk0BBaXSfgjgHOcf+NLgCyKv1gHAyllXq830B/7q7+N88vit8H+70lvvr1pH77yKWNMGrb/2QBvAEdFZL6INHXuMh54zhjzs7F2GGP2Op/7vjEm1RjjMMb8D9iOPfP01AVAY2PMU8aYXGPMLmcMo0vss9QY86HzGGfKeZ1PjDFLjDE5wKPAABFpWXIHEYnEfin90RiTbozZAzwP/NqTQJ2vdzHwkDEm2xizBnjT0+cDvwReMMbsN8acAJ4p+aAx5hNjzE7n3/hb4Ats15pbxpiVxphlxph853t5HftFoUKIJnzlc8aYzcaYccaYZKA79qz9BefDLbHdNi5E5BYRWSMip0TklPO5iWdx6NZAUuHzna/xCNC0xD77PXidon2MMRnYLpekMvskYn9F7C3Rthdo4WGsScAJY0y6F88v+V5KxoGIDBORZc6L5qeA4VTwtxSRjiKyQEQOiUga8LeK9lc1kyZ85VfGmC3YbpLuzqb92K6GUkSkNfZsfBLQyBhTH9iA7ZIA+4vB5eXLbO8Hdhtj6pe4JRhjhlfwHHeKzuZFJB7bZZJaZp9j2O6h1iXaWmG7qzw5TirQUEQSynl+ZQ6WjNP53MKYY7DXMf4JNHX+LRdS8d/yNWwXUQdjTF3sF6W42U/VYJrwlU+JSGcR+b2IJDu3WwJjgGXOXd4EHhCR3s6x5u2dyT4Om4iOOp93G8VfEgCHgWQRiS7T1q7E9nIgTUQeEpHaIhIpIt2rMCR0uHNoaTTwNPCTMabULwNjTAF2FNJfRSTB+R5+h70gXF68JZ+/H/gReEZEYkWkJ/Zi7bsexjgbuE9EkkWkAfaCd6Fo7PWHo0C+iAwDrizx+GGgkYjUK9GWgL2QnSEinbGjrFSI0YSvfC0d6Af8JCKZ2ES/Afg92H564K/ATOe+HwINjTGbsH3gS7EJqQfwQ4nX/RrYCBwSkWPOtv8AXZ3dNx86k/A12Augu7Fn4W9iR9OcjZnAE9iunN7A2HL2uxd7YXkX8L3zedMqiLesMdgLpKnAPOAJY8yXHsb4BvA5sBZYBcwtfMDZTXQf9kvhJHAzdjRQ4eNbgPeAXc6/XRLwgHO/dOdr/8/DOFQNIsboAihKFRKR6dgRLI8FOxalfE3P8JVSKkxowldKqTChXTpKKRUm9AxfKaXCRLUunpaYmGjatGkT7DCUUqrGWLly5TFjTGN3j1XrhN+mTRtWrFgR7DCUUqrGEJG95T2mXTpKKRUmNOErpVSY0ISvlFJholr34buTl5dHSkoK2dnZwQ4lIGJjY0lOTiYqKirYoSilargal/BTUlJISEigTZs2iIR2MT9jDMePHyclJYW2bdsGOxylVA3nky4dEZkmIkdEZEM5jw92Lmi9xnn7U1WPlZ2dTaNGjUI+2QOICI0aNQqbXzNKhbusvCzmb53Pf9f+lwNpnlbK9pyvzvCnY9fAfLuCfb4zxlzti4OFQ7IvFE7vValwlpaTxrgPx7Hv9D4AXlvxGi8OfZELWpxtde/y+eQM3xizBFtKVimlVBV8vPXjomQPkFuQy9SVU316jECO0hkgImtF5FMR6VbeTiIyQURWiMiKo0ePBjA8z/31r3+lW7du9OzZk/POO4+ffvqpwv2nT59OamrZBZOs999/n27duhEREaGTzJQKY0cyj7i2Zbm2eSNQCX8V0NoYcy7wMnbRC7eMMVONMX2MMX0aN3Y7Ozioli5dyoIFC1i1ahXr1q1j0aJFtGzZssLnVJTwu3fvzty5cxk0aJA/wlVK1RCD2wx2abu0zaU+PUZAEr4xJs25GDTGmIVAlIj4f4Hkffvgrrugb18YOxbWrvX6JQ8ePEhiYiIxMTEAJCYmkpRk17deuXIll1xyCb179+aqq67i4MGDzJkzhxUrVjB27FjOO+88zpw5U+r1unTpQqdOnbyOSylVs53f/HweH/Q4SQlJxEfHc32X65l4wUSfHiMgCV9Emonz6qOI9HUe97jfD/zAA7ByJTgcsHUr3H8/lEm4Z+vKK69k//79dOzYkYkTJ/Ltt98Cdn7Avffey5w5c1i5ciW33347jz76KDfccAN9+vTh3XffZc2aNdSuXdsX70wpFYJGdh7J/DHz+WbcN/xx4B+JjnS7JHKV+WSUjoi8BwwGEkUkBbseaBSAMWYKcANwt4jkA2eA0cbfhfhTUmDXrtJtaWmwejVceGGVXzY+Pp6VK1fy3XffsXjxYm666SaeffZZ+vTpw4YNGxgyZAgABQUFNG/e3Jt3oJRSPuWThG+MGVPJ4//GDtsMnAYNICYGcnJKtzdr5vVLR0ZGMnjwYAYPHkyPHj2YMWMGvXv3plu3bixdutTr11dKKX8I3Vo6cXFwxx2l20aMgHbtvHrZrVu3sn379qLtNWvW0Lp1azp16sTRo0eLEn5eXh4bN24EICEhgfT0dK+Oq5RS3qpxpRXOyu23Q79+sGoVdOhg73spIyODe++9l1OnTlGrVi3at2/P1KlTiY6OZs6cOdx3332cPn2a/Px8fvvb39KtWzfGjRvHb37zG2rXrs3SpUtL9ePPmzePe++9l6NHjzJixAjOO+88Pv/8c6/jVEqpsqr1mrZ9+vQxZcemb968mS5dugQpouAIx/eslKoaEVlpjOnj7rHQ7dJRSilViiZ8pZQKE5rwlVIqTGjCV0qpMKEJXymlwoQmfKWUChOa8KvAl+WRH3zwQTp37kzPnj0ZNWoUp06d8kfISimlCf9s+bo88pAhQ9iwYQPr1q2jY8eOPPPMM/4IWymlQjvh7zu9j7s+vou+b/Rl7NyxrD1U/cojX3nlldSqZSc89+/fn5SUFK9jVEopd0I64T/wxQOsPLgSh3Gw9dhW7v/8fs7kVd/yyNOmTWPYsGFexaeUUuUJ2Vo6KWkp7DpZujxyWk4aqw+t5sKW1a888l//+ldq1arF2LFjqxybUkpVJGQTfoPYBsTUiiEnv3R55Gbx1a888owZM1iwYAFfffUVznVilFLK50K2SycuOo47zi9dHnlEhxG0a1C9yiN/9tln/P3vf2f+/PnUqVPHq9hUcOw7vY/NRzdTnQsRKgUhfIYPcPv5t9OvRT9WHVxFh0Yd6Nei+pVHnjRpEjk5OUVdQf3792fKlClex6n8L68gj0e+eoTFexYDcE7Dc3h52Ms0iWsS5MiUck/LI9cA4fiea4K5m+fyt+/+VqptRIcRPHnpk0GKSCktj6yUX2w6usm17Zhrm1LVhSZ8paqoW+NuLm3dG3cPQiRKeaZGJvzq3A3la+H0XmuaazpdwxXtrija7tioIxMvmBjEiJSqWI27aBsbG8vx48dp1KhRyA9hNMZw/PhxYmNjgx2KcqNWRC2eveJZDqQdIDMvk46NOgY7JKUqVOMSfnJyMikpKRw9ejTYoQREbGwsycnJwQ5DVaBF3RbBDkEpj9S4hB8VFUXbtm2DHYZSfve/Df/j420fE1srlrE9xnJp20uDHZKq4WpcwlcqHLy/8X3+8eM/irbXHl7L61e/Tq/mvTx6fm5BLot3LyY9N53BbQaTWCfRX6GqGkQTvlLV0MIdC0ttG2P4dPunHiX8jNwMbv/o9qJaUi/+9CKvDH+Fnk17+iVWVXPUyFE6SoW6OrVcy2zERcd59NyPtnxUqnDgmbwzTFmhs7eVJnylqqVfn/trIqT4f8/46Hiu73K9R889mHHQozYVfrRLR6lqqH9yf6ZfN52F2xcSWyuWUZ1HeTwaaGCrgczaMMulTSmfJHwRmQZcDRwxxrhMNRQ7YP5FYDiQBYwzxqzyxbGVClVdG3ela+OuZ/28fsn9+N2A3/HWmrfIyM1g6DlDq/WEMGMMJ7NPUj+2fqlfNcr3fHWGPx34N/B2OY8PAzo4b/2A15z/KqX84OYeN3Nzj5sxxlTrCYprD63lz9/+mf2n99MsvhmPDnyUAS0HBDuskOWTr1NjzBLgRAW7jATeNtYyoL6IeL4clFKqSqpzss935PPQoofYf3o/AIcyDvHHr/5IVl5WkCMLXYH6/dQC2F9iO8XZ5kJEJojIChFZES6zaZUKtJNnTjJz/UymrZ5GSlpKUGLYdXIXx7KOlWrLyM1g89HNfjumMYY1h9bwU8pP5Dvy/Xac6ipQF23dnWa4rQpmjJkKTAVbD9+fQSkVjo5kHuHX837N8azjALy56k1eG/Ea5zY7N6BxNI9v7rIMaYRE0LJeS78cLysvi0kLJ7Hu8DoAkhKSmHL1FJISkvxyvOooUGf4KUDJTzEZSA3QsZVSJby/8f2iZA92Vu5ba94KeBwJMQlMumBSqW6n8b3G+23FsDmb5hQle4DU9NSwm58QqDP8+cAkEZmFvVh72hijA4NV2MgtyOXLnV9yMOMgF7e6mM6JnYMWy/Ezx13aTpyp6BKc/4zpMYaBrQey/vB6Oid2pm0D/9XJ2nlip0vbjhM7/Ha86shXwzLfAwYDiSKSAjwBRAEYY6YAC7FDMndgh2Xe5ovjKlUT5BXkcefHd7LxiF3U/vWVr/PEJU9wdcergxLPFe2uYP7W+aXaLm97OQ7jIDs/mzpRrrN8/Sm5bjLJdf1fEbZX8158sv2TUm29m/f2+3GrE58kfGPMmEoeN8A9vjiWUjXNkr1LipI92AuHr/78atAS/oUtL+TRgY/y33X/JTs/m2s6XkP92PoMfWcoJ86coHfz3jx92dMhtxj7NZ2uYf2R9czfOh+HcTAgeQB39bkr2GEFVI1bxFypmuZ/G/5XqvIlQGREJMvuWFYthk3uPrmbX875ZanV1QYkD+Dl4S9X+LysvCw+3PIhKWkp9G7emwa1G9Cmfhsa1m7o75C9cjr7NLkFuTSOaxzsUPyiokXMtbSCUn42sPVA/rXsX6WGAV7W5rJqkewBfjrwk8tSmssOLMNhHOXOfM135DN+/ni2Hd9GRm4GTy95mka1G9EsvhmT+k7iVz1/FYjQq6RebL1ghxA0Oo9ZhbwTZ07wvw3/Y/bG2ZzKPhXw4yclJPH8lc/TsVFHEmISGN5hOI8MfCTgcZSnZV3XYZDJdZMrLHPw/b7v2XZ8GwZDanoqDuPg+Jnj5BXk8dJPL3EwXcdkVEd6hq9C2p5Te7j9o9tJy0kD4I1Vb/DWyLcCcpGwpItaXcRFrS4K6DE9NaDlAAa2Gsh3+74DICoyivv731/hc9Jz0gHIL8inwBQA4DAODAaHcbDt+DaaJ+hk+upGE74KaW+vfbso2YOdYfrOund4+OKHgxhV9RIhEUy+ajIrD64kNT2V/sn9K71gO6j1IOKi48jMzaRWRC3yHfnUja5LhEQQGRHJOQ3PISUthaSEJC2IVo1owlch7UjmEY/ayuMwDjJyM6gbU9eXYVU7IkKfJLfX+dyqF1uP10a8xms/v0Z8dDwH0g6QEJNAbK1YLm1zKbfMu4W0nDSaxjflL5f+hfObn+/H6JWnNOGrkDa4zWCWpSwr1XZpG88WA/9x/4888/0zHEw/SLsG7Xhy8JN0adzFH2HWSF0bdy0ayZNbkMuuk7uIi4rjpjk3kVuQC8DhjMM8+vWjLLh5gZ7pVwP6CaiQdn2X67mz1500qN2AhrUbcnefu7mm0zWVPi8jN4OHFj1UdPFx18ldPLToIRzG4e+Qa6ToyGg6J3Zm7+m9Rcm+0JHMI0Er0OYLx7KOubynmkrP8FVIExHu6nPXWU+wWXd4HWfyzpRqS01PJSUthVb1WvkyxJDSrkE7IiSi1BdjQkwCTeOaBjGqqklJS+HhRQ+z5dgW4qLjuOeCe/hlt18GOyyv6Bm+Um60rNvSZZx8nag6JNZJDFJENUNSQhJ39rqzqPsmKjKKBy98kJhaMUGO7Ow99e1TbDm2BYDM3Eye++G5Gl97R8/wlXKjZb2W3Nz9Zt5d/y5gfylM6jsp4HVmaqI7e9/JiI4j2HFiB92bdK/2M2/dcRgHqw66rsK6InUF7Ru2D0JEvqEJX6ly3D/gfoZ3GM72E9s5t+m5fqvTHoqSEpJqdJ35CImgdf3W7D21t1R7uwbtghSRb2jCV6oCnRI70SmxU7DD8IjDOJi9cTZf7/6axDqJjDtvHB0bdQx2WDXWHy78A7//4vdk52cDcNU5V3FB0gVBjso7WjxNqRDx+orXeWPVG0XbdaLqMOeXc0Ku6mUgpeWksSJ1Bc3jm9eYIbkVFU/Ti7ZKhYi5W+aW2s7Ky+LzHZ8HKZrQUDemLpe1vazGJPvKaMJXKkRERUS5tNWK0F5bVUwTvlIh4uYeN5OVl0VaThoFjgIa1G7A0PZDgx2WqkY04SsVAgocBSxLWUa+I5/03HTSctK4q/ddLD+wnPWH11f5dU9ln+JA2gEfRqqCSX/vKRUCFu9ZzI/7f6RuTF3qxtQlIzeDiZ9MLJoVPKTdEP52+d88XnQlJz+HR756hG/2foMgdGvSjeevfF4nntVweoavVAjYc2pPqe1DGYc4k19cGuLLXV/yc+rPgF2t6q3VbzHh4wk89e1T7D+9v9RzVx9czUXTLmLKyilsPbaV41nH2XhkIy8se8Hv70P5lyZ8pUJA3xZ9i+4bY8hz5BEXFVdqn8Ivhed+eI5Xfn6FVQdXMX/rfO6YfwdZeVlFz31qyVMczjxstzEcyTpCTn4Oaw6tKXqtmetnMvSdoQyePpjnf3y+1PKNqvrShK9UCOjZtCf39buPOlF1EBFa1W1Fs/hmRY+LCBckXUC+I58F2xYAkJ2fTWp6KmsOreFv3/0NgNM5p9l/ej+xkbGlXj87P7toEteSvUuYvHQyx7KOkZGbwXsb3mP6mumBeaPKK9qHr1SIuOXcWxjdfTRZeVlk5mbyyNePsPHIRurG1OXevvfStkFbChwFREgEeY489p7eW1TVcvbG2XRo2IFbzr2FpIQkDIa03DQycjMAaFG3Bff2vReAb/Z843Lsb/Z8w/he471+D8sPLGfWhllk52dzXefruPKcK71+TVVME75SISQ6MproyGjqx9ZnxnUzOJV9ivjo+KLx+JERkdzY9Ub+texfRck+KiKK+Oh4Ptj8Abeedyt/uuRP/OHLPyB1hTxHHtd0vIYnLnmCqEg7zt/dzF1fzOZdd3gdkxZOKopr+YHlFDgKGNZhmNevrSxN+ErVECfPnCSmVsxZVeysH1vfpe3efvey8+ROZm+cTXRkNA1rNyRCIopKGvdJ6sOnYz9ly7EtJNdNplGdRqWef2PXG1mwbQGHMg4BtoSDL87u52+d77LAzLwt8zTh+5AmfKWquW/3fMtjix/jYPpBEuskcnOPm7mv331Vfr0IieDxQY+z4ciGUgu839TtpqL7MbViOLfZuW6f36hOI2bdMItFuxZxJu8Ml7e73Cdn+DpT2P/0r6lUNTZz/Uwe/OJBTuWcAuBk9kneXvs2PZr04NK2nq3N607juMa8NfItZq6fyYkzJxhyzpCz6i+Pj47nus7XVfn47vyiyy/4aOtHRcsJigiju4/26THCnSZ8paoph3Hwn9X/ITMvs6gtOz+bjNwMfk792auED9C6fmv+OPCP3oZZZTtP7GTu5rnkOfIY2Wkk3Zp0Y/p103l/4/vkFORwbadr6ZPktuijqiJN+EpVUwWOAtJz0ompFUNebl5Re74jv8YvxLHt+DZu++g2cvJzAPhwy4e8OuJV+iT14dFBjwY5utDlk3H4IjJURLaKyA4RedjN44NF5LSIrHHe/uSL46rQ4TAOZq6fyd0L7uapb59i3+l9wQ4p6KIioxjcZjBN6jQhUiIB2/9+YcsLuabjNUGOzjvvb3y/KNlD8eev/MvrM3wRiQReAYYAKcDPIjLfGLOpzK7fGWOu9vZ4KjS9uOzFovVjwU7umXvTXOrG1A1iVMH3+KDHqRtTlyV7lxAdEc3YnmMZ3X20xzVxqqucghyXtsKVpZT/+KJLpy+wwxizC0BEZgEjgbIJX6lyzdsyr9T2qexTfL37a59fGKxpEmISeGzQY8EOw+eu7ng1n+74lJIr7l3b6dogRhQefJHwWwAlqy+lAP3c7DdARNYCqcADxpiN7l5MRCYAEwBatWrlg/BUTeDujLVwXLgKPX1b9OUfQ/7BrA2zyHfkM6rzKK3dHwC+SPjufluWXSh3FdDaGJMhIsOBD4EO7l7MGDMVmAp2TVsfxKdqgJu63cS01dOKthvHNebytpf7/bgZuRksS1lGw9oN6dW8l9+Pp4oNbjOYwW0GBzuMsOKLhJ8CtCyxnYw9iy9ijEkrcX+hiLwqIonGmGM+OL4KAXf3uZsWCS34bt93NI9vztieY4mLjqv8iV5Yf3g99356b1G9mL4t+vLi0BeLSggoFWp8kfB/BjqISFvgADAauLnkDiLSDDhsjDEi0hc7Oui4D46tQoSIMLLzSEZ2HhmwY77404tFyR5s7ZZFuxZVOpU/MzeTeVvmse/0Pvon9+eytpf5O1SlfMLrhG+MyReRScDnQCQwzRizUUR+43x8CnADcLeI5ANngNGm5NUapYJg96ndHrWVVOAoYMKCCWw9thWAuZvncvv5tzPxgol+idFbu0/u5ps939CoTiOGtBtC7ajawQ5JBZFPJl4ZYxYCC8u0TSlx/9/Av31xLKV8pW9SX77c9WWptn4t3I03KLYsZVlRsi/03ob3GN9rPNGR0T6P0Rvf7vmWB798sKgg2Tvr3mH6ddPPqviav208spHDmYfpk9Sn2gzBTctJ473177Hr5C4uaHEBozqPIjIiMthh+YTOtFVh64ELH+Bk9klWpK6gTlQdxp03jt5JvSt8TskyB4Vy8nPId+RXu4Q/ZeWUUtUnd53cxec7PmdUl1EVPu/EmRO8uepNNh/bTI8mPRjfa7zPk7HDOHh40cN8vftrwFbcnHzV5KCXUnAYBxM+nsCOEzsA+Gr3V2w9tjVkZv9qwldhq1GdRky5egqns09TO6q2Rwn7opYXUS+2HqezTxe1DW4zuFqdNRc6luU6JuJo1tEKn2OMYdLCSWw7vg2wF7Y3H93MG9e+4dPYftz/Y1GyB8jKy2Ly0snMvD64s22XH1helOwLzd82n//r/3/ER8cHKSrf0YHOKuzVi63n8dl5XHQcr1/9OoPbDKZtg7aM6T6GPw/+s38DxJ55rjm0xqU7qSK9mvfiYMZB9p3ex8nsk0RIRKXDIDcf21yU7AutPrTa56Uudp3c5dK28+ROnx6jKtytzWuMcanTX1PpGb5SZ6l9w/b888p/Bux4hzMOM3HhRPae2gvY4aOTr5pMbK3Ycp+TnpPOigMrwMCZ/DPkOfK4st2VRevSlqe8+vOe1qXfdnwbryx/hd2ndtO3RV/u63ef2+4gd103FyRd4NEx/Klfi34kJSSRml48svzSNpdWm+sL3tIzfKWquddXvl6U7MF2O8zdPLfC5yzes5jTOadpntCcTo06cU6Dc9h4dCOVDY7r2KijSzK+pPUlJCUkVRpnVl4Wd39yNz/s/4HU9FQ+3PIhj33tvixE18Zdub///UVdYV0bd+WRgY9Uegx/i4qM4vWrX+e6ztfRs2lP7jj/Dp689Mlgh+UzeoavVDW35dgWl7az6dopyZOia/+66l/M3ji76KLtjd1u9Oi1l6UsK3VtA2BpylLSctLcniGP7TmWG7reQHpuOol1Ej17AwHQPKF5SNYvAk34SlV75zY916VfvbzlBwtd1vYyXv351VIXbm/s6lnirh1Vm1vPu/Ws43R3UbNwUfXyxNSKIaZWzFkfS1WNJnylqrEvdn7BlmNbyMzNxIGDujF1GXrOUEZ2qnhGcnx0PNNGTuOdde+Qmp7KwFYDKx2O6a0+SX3o2bQn6w6vK2ob22Ns0bWGA2kHOHHmBN2adNPCeEEi1XnCa58+fcyKFSuCHYZSQfH17q/5w5d/KNrOLcjlmcufYUTHEUGMqmLZ+dks2LaA3Sd30y+5H4NaD8JhHDz97dN8vO1jAJISknhp2Eu0qd8muMGGKBFZaYxxO6FBv2aVqqbmb51fajs6Mprv9n0XpGg8E1srlhu63sCDFz3IoNaDADvjtzDZA6SmpzJ56eRghRjWNOErFSBpOWnkFeRVvqNTTKRr37a7tupu87HNLm0bj7pdDkP5mSZ8pfzsSOYRJnw8gctmXMaV71zJrA2zPHre6O6jS9VwiYqM4qbuN/krTL/p3qS7S1uPJj2CEInSPnyl/OhI5hFu+/A2tsdhCaEAAB5oSURBVB7fSt2YukUXK98e9TZdG3et9Pmbjm7iwy0fEiER/KLLLyqdOFUdGWN49vtnmbdlHg7joFW9Vrw49EVa1mtZ+ZPVWauoD18TvlJ+su7wOu5ZeA+rD67GYIiOiKZN/TZERkRyzwX3cNv5twU7xIA6knmEU9mn6NCwQ41fhL0604u2SgXBGyvf4EzemaJ+91xHLiezTwKE5QiVJnFN6Niooyb7INKEr5SfHM48DECT+CaIc+nnfEc+F7W8iEvaXBLM0FSY0olXSvnJoNaD2HVyF3FRcXRo1IHM3EweufgRxp0/LtihqTClCV8pP5nQewLpOel8sv0T4qLjGHfhOMb0GBPssFQY04u2SikVQvSirVJKKU34SikVLrQPXynFyTMnOZRxiA6NOni8ulVNt/rgar7c9SX1Y+szqvMoGsc1DnZIfhcen6xSZyHfkc+hjEM0i28WFsnvzVVv8uaqN8l35NM4rjH/GPIPt+UQQslnOz7j8cWPF60ANnfzXGZeP5OGtRsGOTL/0i4dpUpYun8pI2aO4LpZ1zH83eEs2bsk2CH51fbj25myYkrR4t1HM4/y9JKngxyV/81YO6PUco/Hso6xYNuCIEYUGJrwA2X5cpg2DX76KdiRqHLk5Ofw6NePcjzrOAAnzpzg8cWPk5WXFeTI/Gf9kfUubTtP7ORM3pkgRBM4mbmZHrWFGk34gfDPf8LEifDqq3DPPfDcc8GOSLmx8+RO0nLSSrVl5ma6LC8YSro17ubS1rZBW2pH1Q5CNIEzvMPwUtu1ImpxVfurghRN4GjC97cjR2D27NJtc+bA4cPBiSfMZeZmMnnpZMZ8MIZHvnqEfaf3FT2WXDfZZX3VqMgoWtdrHegwA6ZTYiduP//2oiqeDWo34NGBjwY5Kv+b0HsCEy+YSPuG7emT1IcXh75Iuwbtgh2W3+nEK3/btAluucW1fcYM6OZ6dgWAMeBwQGSk+8dVlf3u89+V6pdvHNeYj0Z/VLTQ9pxNc/jHj/+gwFFAhETw2/6/5eYeNwcr3IA5knmEg+kH6dK4S4WLjqvqr6KJV6E/BCHYOneGpCRITS1ua9YMunRxv/+cOTB1Kpw6BYMHw2OPQd26AQk11J3OPu2yRODRzKMsS1lWtBzfDV1vYFDrQWw5toVOjTrRNL5pMEINuCZxTWgS1yTYYSg/80mXjogMFZGtIrJDRB5287iIyEvOx9eJSC9fHLdGiIiAf/0LevWCmBj77wsv2Pay1q+HZ5+FEyfsGf7XX8NkXfvTV6Iio9wOs6xdq3R/dZO4JgxqPShskr0KH16f4YtIJPAKMARIAX4WkfnGmE0ldhsGdHDe+gGvOf8ND+ecY8/aK/PDD561qSqpE1WH67tcX2qJwa6Nu9I7qXcQo1IqcHzRpdMX2GGM2QUgIrOAkUDJhD8SeNvYCwbLRKS+iDQ3xhz0wfFDR1KSZ22qyn4/4Pd0bdyVnw/8TNsGbbm+y/VFFyyVCnW+SPgtgP0ltlNwPXt3t08LwCXhi8gEYAJAq1atfBBeDXLVVTB3LmzYYLejo+0wTuUzIsLwDsNdhuUpFQ58kfDdrVdWduiPJ/vYRmOmAlPBjtLxLrQaJiYG/vMf+P57OH4cBg6ExqFf30MpFRi+SPgpQMnl55OB1Crso8AOxbxEl79TSvmeLzovfwY6iEhbEYkGRgPzy+wzH7jFOVqnP3Ba+++VUiqwvD7DN8bki8gk4HMgEphmjNkoIr9xPj4FWAgMB3YAWcBt3h5XKaXU2fHJxCtjzEJsUi/ZNqXEfQPo1cfqLj0djh6FNm3czxNQKkhWH1zNu+vfJSsvi6s7Xq0X3atIZ9r6Um4uFBRA7RpYeOqdd+C11yAnx84Efu456No12FEpxdZjW7n7k7uLSjgvP7CcvII8RnYeGeTIah49jfMFY+xs2sGD7QXXRx6B7OxgR+W53bvt7N+cHLt96BA8+WRwY1LK6eNtHxcl+0Jzt8wNUjQ1myZ8X1i4EN59157hOxzwxRd2eGVlsrPtmPu0tMr39af1rjXR2bkTMkO/Priq/iLFtYhgLdHOiarQhO8Ly5a5tlW20MkPP8CwYTBuHAwd6lpCOZA6dXJta9kS6tQJfCzKpxzGEewQvDaqyyhia8WWahvdfXSQoqnZNOH7grsZwRXNEs7Pt10m6el2OzcXnn/e1s73l61bYd06+wukrE6dbAnnwgu18fHwxz+CuJsvp2qCnSd2Mu7DcfR9oy83zbmJNYfWBDukKmtTvw3Tr5vOL7r8gqHth/LysJcZcs6QYIdVI2k9fF9IS4Px42HXLrvdqBG88Ub5SX//fhg1yrV98mQYNMh3cTkc8P77tgLn8ePQsCF06ACvvAJN3JTCTU2FlBTo0aNmXnhWgD2rv3729ew/XVzNpG5MXRaOXehypqxCj9bD97e6dWHmTFi61F74vOiiihNm8+Y2+Z44UdwWGWlr5/vSCy/Av/9dvLpWero9zpQp8Kc/ue6flKTF2kLA3lN7SyV7gLScNNYdXkffFn2DFJWqDrRLx1dq1bK1b664ovKz41q1bMKNj7fb0dHw+9+7P+uuqtxcu5hKydFCubmQkWG7d1TISqyT6HbVqmbxzYIQjapONOEHy8UXw6efwrRp9t9f/tK3r1+4TGLZLx+Hw37RzJ5dehUuFTISYhIY32t8qbYbut5Aq3phVn1WudA+/FD21FPw0Uc2sael2e6cpk1two+MtL80/vlP++WjQs6WY1tYc2gNnRp14vzm5wc7HBUg2ocfrh5+GFq0sNcWGja09//73+LH8/Ph5Zc14Yeozomd6Zzo4+tCqkbThB9K1q+3M3537oQ+feChh+COO+wNbPdRWQerQdHSzEz46it7jeGyy+yXk1LK5zThV9WZM7B3L7RuXT2GMGZlwX33FY/t//ZbOHmydJK/+GJbL6dkN16wz+6PHrWTzwpHEr3yih3S2r59UMNSKhTpRduqWLTIzo791a/sbNlFi4IdESxfXpzsC61bB8eOFW937AhPPGFHA0VE2LPphx4KbJxlzZpVnOzBvgd3v0SUUl7TM/yzlZVlL4ZmZdntjAy7feGFwS1FkJjo2hYbC3FxpduuvtreHI7qUQL50CHP2gqlpcEnn9hfL1dcYb/ElFIeqQb/x9cwO3YUJ/tCWVm23zyYund3naV7223ldzeVTPYHD9qulWBwN7O4vNnGaWn2V9Xzz9tfAb/6le26Ukp5RM/wz1bbtnax8cJSwmDPpNu2DV5Mhf7xD/jmm+KLtr16Vbx/Zqbt0lm2zNbNueIK+2slKsqz4x05Ykf6eDM796qr4MABW200Lw9GjoRf/9r9vgsWlJ474HDAm2/qGsBKeUgT/tlKSLBJ8tln7aiS6Gi7XThrNpgiI+Hyy+3NE//5T3GlT2Pgyy+hZ08YM8a27doFixdDgwb2mkVhl1Vurp0p/NVX9nl9+9oFU6r6N7j9dnurzKlTrm0nT1btmEqFIU34VXHttfascudOO5qkbl3vXi893SZWsBdSA/XlscZNBcXVq23CX7IEHnzQruAF9gz87bftNYE5c0pfqF6+3Hax3Heff+O9/HKYPr10xc8rr/TvMZUKIdqHX1X16tkuE2+TfUoKXH+97Up56il7P1AlD9zVwS9smzq1ONmDHYL6ySf2/rp1rs9z9+Xha5062V9WnTrZGcO//jXcfbf/j6tUiNCEH2xvv126aubx43Z92UC44w5o1654u2dPGO1cWKLkcM6SsYH7kTHuvjz84bLL7K+NTz6B//s/z683KKW0Syfo3M10DdQZfmKiHQe/bp2tq9O9e/Fjl18O//tf8XZEBFx6qb0/ejR8/33xmX67dsWzeZVS1ZYm/GAbONDWuinbFigREXDeea7t991nL8h+8YUtdTB+fHG9/jp1bJ/9pk12ZE2PHtVjTL9SqkJaLTOQMjLsSJqSY+MdDrtIyZw5dvvGG+GeewKTQPPzYc8euyBL2QlaSqkaqaJqmZrwAyE7265h+9VXNuGPGgUPPFA6qRsT2DVkV6+269amptrx+IMH27ILFa3F6w+nTsG2bXa0kxZNU8prFSV8/R0eCNOm2THuDoftApk9Gz7+uPQ+gUz2DodN7ikpdqz9oUO2L//GG23yDZSPP4bhw2HiRBgxAj74IHDHVioMacIPhJ9+8qwtUI4ft2f2J06Urpx58qRdmzcQMjPtZK3cXLudl2dLJpw+HZjjKxWGNOFXZM8em4T+8hfbBVJVbdq4tgWzFEPDhtC4celx9mBLRBRW3Dxxwg5/fPNN2L/f9TW8tW+fLTFdUm6u/cWhlPILrxK+iDQUkS9FZLvz3wbl7LdHRNaLyBoRqRmd8vv2wS23wHvvwYcfwoQJdvZpVdx5Z+kFys85B266yTdxVkVkJDzySOmYGje2ZSKGDbM1csaMsYupTJliY1271rcxtG1ry1SUVLs2dOjg2+MopYp4e4b/MPCVMaYD8JVzuzyXGmPOK+9iQrXzwQelq2IaU/XujuRkmDfPrh/70kv2dbydoeutgQNtt9ITT9gCZhdeCI89ZguozZlTPMkK7Jm3r2vUx8baC9mFf4eEBFufpzrUJFIqRHk7Dn8kMNh5fwbwDRDkFTV8pGQ1zELZ2VV/vZgYOxKmKr780pYBLuwGadHCTnTq0aPq8YAdT/+HP7i2l0z2hUrOBi7rzBn47js7eevii+0vBU8MGgSffWbfV3Ky/RJQSvmNtwm/qTHmIIAx5qCINClnPwN8ISIGeN0YM9XL4/rf1VfD3LmlC3Vde23g43jjDXj9dbsq1IkTtpTAOefAihXw/vt2DL2vXXEFfPRR6bYhQ9zve+CAnZRVWE8/Odn+GvB0iGV0tC5nqFSAVNqlIyKLRGSDm9vIszjORcaYXsAw4B4RKWeFCxCRCSKyQkRWHA3Wohxgywy8+CL07w/nnmu7O37xi8DGYIy9cArFo1fy8uyF1exs/y2tOGCAfb+tW9siZePH28VG3Jk2rfTiKSkpxTErpaqVSs/wjTFXlPeYiBwWkebOs/vmwJFyXiPV+e8REZkH9AXcXgF1nv1PBTvxqvK34EedOtkKlsGaEGRM8bDFiIjiUTWFvzr8uXj6ddfZW2XcjeDxx6gepZTXvL1oOx+41Xn/VuCjsjuISJyIJBTeB64ENnh5XP/KzbWzUK+6yt4efrg48QZSRITtWoLiL53ISHuBs0kTz2vBZ2baIaYlu6e8cfy4XQvAGPsLqCx3be5kZ9t1AJYu9V1sSqlyeduH/ywwW0TuAPYBNwKISBLwpjFmONAUmCd2JmktYKYx5jMvj+tf775rL5QWWrTInu3fdlvgY3ngAVvV8vvvbVJMTLRFzG64wbORPjNnwquv2uSalGQnOxUWQTtbxthlFOfMsbG0bWtfLyUFFi60X1DXX+/ZL4N9++xQ18IyzB072hr8OkpHKb/xKuEbY44DLuvpObtwhjvv7wLO9eY4AbdqlWvbypXBSfi5ufYWGwtdutilAOvV8+y5e/bA5MnF26mpdijke+9VLZYffrBlIQrt3m2vc7zwgl3mUcTzETpTppSuub9tm/0iGTeuarEppSql5ZHdad/etWRxsEaSPPCAHZED9oto9Wq7aIon3E2W2rbNTiTbtctemL7iCs8rc7pb6aqwLSbGs9cotGePa9vu3Wf3Gkqps6IJ351f/9qezRZO82/Xzs66DbR9+4qTfaFNm2DzZnu2Xxl3s1YzMmypiEI//gh//rNn8bg7pidxuNOvn2uhtn79qvZaSimPaC0ddxo2tNUjX3vN3mbNCs5IncjIs2svq2tXuPnm4kqctWq5ns1/8omtlumJSy6x1S0LNW0Kv/+9Z88t68477XKFERG2G+jmm21ZB6WU32g9/Opo7147C7ZxY7tu6w8/FD92/vl2MhbYMfl5eXbfiqSk2FtUFNx1l+vjM2fai6ZbtsD27XbeQUV18ffts5PAevTw/MunPOnp9ovIn0NMg2HJEvjmG/uleOONWutfBUxF9fC1S6c6OXYMfvc7221TOCTzmWfs2rLr19vuk5tvtvtOmwYzZth6PwMH2ouxZYuRFUpOtjeHwybyffuKH2vXznb9TJ5cXCtIBB58EH75S/ev16qV7xZKKS/mmuzdd23huUILFthfibqqmAoy7dKpTl5+2SZ7sMl5/nxbQ+e222xCvvNOmzSWLrVDLTMz7VDJJUtsUbbKRETYY1x6qT3zvOwyO8Jm//7SI3eMscsuliwepzz33/+W3j540H+zopU6C3qGX52UNwqmbN/2smWu+5UdVVSeFi3sWPqSfvyx9EIoYJP9sWOBX/IwFJSt819em1IBpmf41UmnTp61uUvCrVrZXwWLF9vunvXrPT9uz56u1wEKu4HU2bvmmtLbderA5S7TVZQKOD3Dr07uvddeOE1JsduDBtm1XssaMcKuB7vBWaEiPh7uuceO2S+5SMtvf1t+0bOS4uPh2WftLTXVVuN88knPx+er0n77Wzs57ttvbQmM8ePtBXilgkxH6VQ3DoftxklIsIm3rKNH4e9/t4uXxMXZ5H/rrXYi0+23l943Ls6WiPB09qvDYa8LhOKFVKXCREWjdPQUrrqJiIDzznOf7AEefdQO9ztzxvaxv/++bT/iplBpZqa9nc2xC5P9/Pl2RNCNNwZuYXOllF9pl05NkpbmWucnK8texO3f3/YVlxxZc+650MDtMsMVW7LEloUuNHmyHSc/apSNYf9+W2ribMspKKWCSs/wa5LYWPdjuRMTbeXMF16wY/VjY+3Y/GeeqdpxPv/cfdvs2TB0qO1CGjbMju7xVH6+HUm0fLmWQlYqSELvDP+nn+Crr+zMxuuvrz4Xy7Ky7Nl506bua9x4IjralhQuOannwguhVy97v1cv1zHgVeGuGmdkpF2EvTBZp6XZC7uffGJnylbkyBE7w7dwYZT27e2yjZ5W/VRK+URoJfyPPoKnny69PWtW8BPL2rV25EZ6ut0eOtTGKWJLHxvjeffI2LF2GOXy5dCmTdUXRnfn9Gkb40032fr2GRm2PToa+vSxX6YlHT9u19pt0aLi133rrdKrYO3YYWejTpzou9iVUpUKrYQ/Y0bp7aNHbeIaMyY48RT65z+Lkz3AZ5/ZpL98OXzwgV26cPhwu7KWJyNqevSwN1+aPNmWcCgosN1CL71krw3k59vYate2heRKdsc0bGh/sVRm507P2pRSfhVaffjVdYbj9u2ube+/b0e/5OTYpDp/vud17n3tu+9sLIVr5m7eDO+8Y7uPJk60vySaNoX77y/+QoqPhz/9qfLuHLC/DjxpU0r5VWgl/ML1XwtFR8OQIcGJpaTCPvaSCrtLSvK0PIKvrVnjWduYMfYX07Rp8OmncPHFnr3+rbfamaYidujniBF2uKdSKqBCq0vnN7+xXQ+LFkGjRnYiUsuWwY4KHnnEVp/cvt321d9xh/3lUXZFqmDVrenY0bXNXUkHgPr17e1sxMTYyWKnTtmE78lavEopn9OZtoGUmmqTZZ06tp787bfbUsUREfYL6s03g/MFVVBgrx8sXmy3Gze21TLLm/yllKq2tB5+dZGUVHz/ww/tTNnMTHuRdMoUaNYssPGsWWMXV2nRwi57uG8fnDxpF1mJigpsLEopv9OEHwzLl9t69mBLGaSk2ITv6dqyvjB7Njz3XPH2vHm2b77sClYOhxZRUypE6P/JweCunn3ZMe7+5HDA1Kml2zZuhO+/L95euNAOx+zf315/OH06cPH5Un4+bN1qJ4opFeb0DD8YWrf2rM1fCgrcJ8CTJ+2/27fDE08UL4qyeLHt4vnb33wbR1aWvVbw/fe2W+nuu+2kMl9Ztw4eesjOx4iOthf1b7nFd6+vVA2jZ/jBMGyYrYhZKD4eJk0K3PGjolxn6BbW3wHbr1/2Yv533/k+jr/+1XYtpabCzz/bv8GJE755bWNs6YejR+12bq6dTLZnj29eX6kaSM/wgyE62naprFhhhyoOGBD4GvSPP26/aAov2k6aZEcKATRvbv/NybGJMza2uM1XCgpc13nNyrKVOq+7zvvXT0+HvXtd2zdssBPJlApDmvCDJSIC+vYN3vETEuxMWXcGDLCjh/bts9t16tizZV+KiLBfOGWvDfhqjH5Cgv0iO3CgdHuXLr55faVqIO3SUa7mzbNJPjnZllRo3hw2bYI33rA1gEaM8L4MhIjrCl0dOxZ3K3lLBB57rPgLJCLCLjWocwtUGNMzfOVq40abMEt2M336aekLvS+9ZCdoDRtW9eOMHWtLJf/wg52jcO21vh3/f8EFdrTRpk32y6tJE9+9tlI1kFdn+CJyo4hsFBGHiJRbDUtEhorIVhHZISIPe3NMFQDdu7u25eS4tn39tffH6tcPfvc7GD3a/qrwtdhYW8tIk71SXnfpbAB+ASwpbwcRiQReAYYBXYExItLVy+Mqf7rpJrjoouLtnj3dF0qrLovLKKU84lWXjjFmM4CIVLRbX2CHMWaXc99ZwEhgkzfHVn4UEwMvvmgv2ubnQ7t2dtGSZcuKF0WvV88ucq6UqjEC0YffAiix3BEpQL8AHFd5q2T1zvbtbQ3/L76wF0CHDrULoCilaoxKE76ILALcVfV61BjzkQfHcHf6X26JThGZAEwAaBWscsHl2bjRXmBMToYrrvBsdapQ0qQJ/OpXwY5CKVVFlSZ8Y8wVXh4jBShZ8zcZSK3geFOBqWDLI3t5bN+ZMweefbZ4+4MP7DBFLSymlKohApGtfgY6iEhbEYkGRgPzA3Bc33E44PXXS7etXQs//hiceJRSqgq8HZY5SkRSgAHAJyLyubM9SUQWAhhj8oFJwOfAZmC2MWajd2EHWH6++2qRx44FPhallKoib0fpzAPmuWlPBYaX2F4ILPTmWEEVHW2HJS5Z4tqmlFI1hHZAe+qJJ+zIlIQEW4/lhRcgMTHYUfnXhg12UtQtt8D06bZrSylVY2lpBU/Vq2eXAQwXqam2fnx2tt3etAkyMgJbxlkp5VN6hq/cW7SoONkXml+zrrUrpUrThK/ci4lxbYuNDXwcSimf0YSv3Bs61LVWjk66UqpG0z585V69ejBjhi2ncOwYXHaZ72rVK6WCQhO+Kl+TJnDPPcGOQinlI9qlo5RSYUITvlJKhQlN+EopFSY04SulVJjQhK+UUmFCE75SSoUJMab6rDFSlogcBfYG4FCJQDjVOg6n9xtO7xX0/YY6T95va2NMY3cPVOuEHygissIY0yfYcQRKOL3fcHqvoO831Hn7frVLRymlwoQmfKWUChOa8K2pwQ4gwMLp/YbTewV9v6HOq/erffhKKRUm9AxfKaXChCZ8pZQKE2GZ8EXkRhHZKCIOESl3iJOIDBWRrSKyQ0QeDmSMviQiDUXkSxHZ7vy3QTn77RGR9SKyRkRWBDpOb1T2WYn1kvPxdSLSKxhx+ooH73ewiJx2fpZrRORPwYjTF0RkmogcEZEN5Tweap9tZe+36p+tMSbsbkAXoBPwDdCnnH0igZ1AOyAaWAt0DXbsVXy/zwEPO+8/DPy9nP32AInBjrcK76/SzwoYDnwKCNAf+CnYcfv5/Q4GFgQ7Vh+930FAL2BDOY+HzGfr4fut8mcblmf4xpjNxpitlezWF9hhjNlljMkFZgEj/R+dX4wEZjjvzwCuC2Is/uDJZzUSeNtYy4D6ItI80IH6SCj9t1kpY8wS4EQFu4TSZ+vJ+62ysEz4HmoB7C+xneJsq4maGmMOAjj/bVLOfgb4QkRWisiEgEXnPU8+q1D6PD19LwNEZK2IfCoi3QITWlCE0mfrqSp9tiG7xKGILAKauXnoUWPMR568hJu2ajuGtaL3exYvc5ExJlVEmgBfisgW59lGdefJZ1WjPs9KePJeVmFrqmSIyHDgQ6CD3yMLjlD6bD1R5c82ZBO+MeYKL18iBWhZYjsZSPXyNf2movcrIodFpLkx5qDzp+6Rcl4j1fnvERGZh+06qAkJ35PPqkZ9npWo9L0YY9JK3F8oIq+KSKIxJhQLjYXSZ1spbz5b7dIp389ABxFpKyLRwGhgfpBjqqr5wK3O+7cCLr9wRCRORBIK7wNXAm5HCVRDnnxW84FbnCM6+gOnC7u5aqBK36+INBMRcd7vi/1//XjAIw2MUPpsK+XNZxuyZ/gVEZFRwMtAY+ATEVljjLlKRJKAN40xw40x+SIyCfgcOypimjFmYxDD9sazwGwRuQPYB9wIUPL9Ak2Bec7/jmoBM40xnwUp3rNS3mclIr9xPj4FWIgdzbEDyAJuC1a83vLw/d4A3C0i+cAZYLRxDvGoaUTkPezIlEQRSQGeAKIg9D5b8Oj9Vvmz1dIKSikVJrRLRymlwoQmfKWUChOa8JVSKkxowldKqTChCV8ppcKEJnyllAoTmvCVUipM/D/0/ZfwjE1QRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate 2-dimensional data for the training now\n",
    "K = 150\n",
    "trainK = 100\n",
    "testK = K-trainK\n",
    "\n",
    "y = np.ones(K)\n",
    "X = np.ones((2,K))\n",
    "stdData = 0.3\n",
    "scaleFact = 0.6\n",
    "X[:,0:K//2] = (scaleFact*np.array([-1,-1])).reshape(2,1)+stdData*np.random.randn(2,K//2)\n",
    "X[:,K//2:] = (scaleFact*np.array([1,1])).reshape(2,1)+stdData*np.random.randn(2,K//2)\n",
    "y[K//2:] = 0\n",
    "\n",
    "# Shuffle the data randomly and divide into training and test sets\n",
    "shuffleIdx = np.random.permutation(K)\n",
    "X = X[:,shuffleIdx]\n",
    "y = y[shuffleIdx]\n",
    "\n",
    "# Training set\n",
    "XTrain = X[:,0:trainK]\n",
    "yTrain = y[0:trainK]\n",
    "\n",
    "# Test set\n",
    "XTest = X[:,trainK:]\n",
    "yTest = y[trainK:]\n",
    "\n",
    "# Check if simulated data is alright\n",
    "grp1 = XTrain[:,np.where(yTrain==1)]\n",
    "grp2 = XTrain[:,np.where(yTrain==0)]\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure(); plt.clf()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(grp1[0,:], grp1[1,:], alpha=0.8, c=\"red\", edgecolors='none', s=30, label=\"Set 1\")\n",
    "ax.scatter(grp2[0,:], grp2[1,:], alpha=0.8, c=\"green\", edgecolors='none', s=30, label=\"Set 2\")\n",
    "\n",
    "ax.set_title('Scatter plot of data')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1. Log-likelihood: -19.221\n",
      "Iteration 2. Log-likelihood: -8.677\n",
      "Iteration 3. Log-likelihood: -4.238\n",
      "Iteration 4. Log-likelihood: -2.109\n",
      "Iteration 5. Log-likelihood: -1.023\n",
      "Iteration 6. Log-likelihood: -0.470\n",
      "Iteration 7. Log-likelihood: -0.204\n",
      "Iteration 8. Log-likelihood: -0.089\n",
      "Iteration 9. Log-likelihood: -0.042\n",
      "[ -2.56691799 -14.52390723 -19.46951185]\n",
      "TP(%): 100.00, FP(%): 0.00, FN(%): 0.00, TN(%): 100.00\n",
      "None\n",
      "[[28.  0.]\n",
      " [ 0. 22.]]\n"
     ]
    }
   ],
   "source": [
    "# Include your code here on training and testing the classifiers.\n",
    "# Don't forget to ensure that the feature vector is augmented during training and testing\n",
    "thetaVec = LogisticRegressionNewton(XTrain,yTrain,augmentFeatures=True,dbgFlag=True)\n",
    "\n",
    "print(thetaVec)\n",
    "\n",
    "# Predict the label for the test data.\n",
    "XTest_aug = np.append(np.ones((1,XTest.shape[1])),XTest,axis=0)\n",
    "yHat = PredictLabel(XTest_aug,thetaVec)\n",
    "\n",
    "# Include the confusion matrix and performance metrics in your summary of the classifier performance.\n",
    "C = GetConfusionMatrix(yTest,yHat)\n",
    "print(ComputeMetrics(C))\n",
    "print(C)\n",
    "# TIP: you should obtain TN and TP rates > 95% for this synthetic case...\n",
    "\n",
    "# Report your results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<span style=\"color:black\">\n",
    "  \n",
    "$$\\boldsymbol{\\mathcal{C}} = \\begin{pmatrix}28 & 0\\\\0 & 22 \\end{pmatrix}$$\n",
    "    \n",
    "Performance metrics:\n",
    "    \n",
    "TP(%): 100.00, FP(%): 0.00, FN(%): 0.00, TN(%): 100.00\n",
    "    \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world data\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. \n",
    "The dataset has been slightly modified from the original and is split, for your convenience, into the `train` and `test` sets.\n",
    "\n",
    "#### Read data from the training file and train the classifier.\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "* Use the code below to read the data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from the diabetes test file.\n",
    "# This is the training data. First fit the classifier parameters on this training data.\n",
    "XTrain = []; yTrain = []\n",
    "fr = open('./diabetesTrain.csv')\n",
    "\n",
    "for line in fr.readlines():\n",
    "    lineArr = line.strip().split(',')\n",
    "    \n",
    "    # Get the dimension of each line. This contains the features (diagnostic measurements)\n",
    "    # for each subject and a label (diabetic or not). The label is the last element of each row\n",
    "    nElements = len(lineArr)  \n",
    "\n",
    "    yTrain.append(float(lineArr[-1]))  # The outcome is the last element.\n",
    "    \n",
    "    # Now append the data to the M-dimensional feature matrix.\n",
    "    featVec = []\n",
    "    for i in lineArr[0:-1]:\n",
    "        featVec.append(float(i))\n",
    "    XTrain.append(featVec)\n",
    "\n",
    "XTrain = np.array(XTrain).T  # Dimension should be N x M (#features x #data-points)\n",
    "yTrain = np.array(yTrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "* Use the _Newton's_ approach to train the classifier. \n",
    "* Use an intercept (offset/bias in the exponent term) in training the classifier (i.e., augment the training data).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1. Log-likelihood: -299.160\n",
      "Iteration 2. Log-likelihood: -289.338\n",
      "Iteration 3. Log-likelihood: -288.924\n",
      "Iteration 4. Log-likelihood: -288.922\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier on the training data.\n",
    "# Don't forget to augment the data!\n",
    "theta = LogisticRegressionNewton(XTrain,yTrain,augmentFeatures=True,dbgFlag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nave classifier\n",
    "\n",
    "The most rudimentary classifier is one that simply makes a _majority_ decision. In other words, if the majority of the population in the training set belong to a particular class, each element of the test set is set to that _majority_ class. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Let us calculate the nave decision error rate. Use following code snippet for this:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive error: 34.85%.\n"
     ]
    }
   ],
   "source": [
    "# This depends on the prevalance of the condition in the population\n",
    "Ph1 = np.sum(yTrain)/np.size(yTrain) #Ph1: a priori probability that a person has the condition\n",
    "# Error rate = 1-np.maximum(Ph1,1-Ph1)\n",
    "naiveErr = (1-np.maximum(Ph1,1-Ph1))*100 # Expressing in percentage\n",
    "print(\"Naive error: %2.2f%%.\"%naiveErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in order for our classifier to be useful, its error rate should be better than this. \n",
    "\n",
    "<div class = \"alert alert-info\" >\n",
    "\n",
    "* Compute the confusion matrix of your trained classifier on _`XTrain`_. This is the error on the training data. This will show that real life data can usually never be perfectly classified and that some error is inherent.\n",
    "* Compute the percentage of _total false decisions_ (**Tip:** this should be in the range of 20\\% - 25\\% or lower)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of logistic regression: 22.48%.\n"
     ]
    }
   ],
   "source": [
    "# Error rate of the trained classifier (% of wrong decisions) on the training data\n",
    "XTrain_aug = np.append(np.ones((1,XTrain.shape[1])),XTrain,axis=0)\n",
    "yHat = PredictLabel(XTrain_aug,theta)\n",
    "confMatrix = GetConfusionMatrix(yTrain,yHat)\n",
    "errCount = confMatrix[1,0]+confMatrix[0,1]\n",
    "errRate =  100*errCount/len(XTrain[0]) # Percentage over total number of training data points\n",
    "print(\"Error of logistic regression: %2.2f%%.\"%errRate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now test on the test data\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "* First read the data from the `diabetesTest.csv` file in a similar manner as done for the training data. Use the snippet below for this.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from the modified diabetes test file.\n",
    "# This is the training data. First fit the classifier parameters on this training data.\n",
    "XTest = []; yTest = []\n",
    "fr = open('./diabetesTest.csv')\n",
    "for line in fr.readlines():\n",
    "    lineArr = line.strip().split(',')\n",
    "    nElements = len(lineArr)  # Get the dimension of each line.\n",
    "\n",
    "    yTest.append(float(lineArr[-1]))  # The outcome is the last element.\n",
    "    \n",
    "    # Now append the data to the M-dimensional feature matrix.\n",
    "    featVec = []\n",
    "    for i in lineArr[0:-1]:\n",
    "        featVec.append(float(i))\n",
    "    XTest.append(featVec)\n",
    "    \n",
    "\n",
    "XTest = np.array(XTest).T  # Dimension should be M x K (#features x #data-points)\n",
    "yTest = np.array(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "* Now apply the trained classifier to predict the label of each test datapoint\n",
    "* Compute the confusion matrix and the performance metrics. List them below. (**Tip:** if all is correct, you should obtain a TP rate of around 61\\% and a TN rate of around 86\\%)\n",
    "* Compare the error rate to the error rate of the nave classifier obtained on the training data. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86. 21.]\n",
      " [14. 33.]]\n",
      "TP(%): 61.11, FP(%): 14.00, FN(%): 38.89, TN(%): 86.00\n",
      "Error of logistic regression: 22.73%.\n"
     ]
    }
   ],
   "source": [
    "# Predicted label using logistic regression classifier trained previously\n",
    "XTest_aug = np.append(np.ones((1,XTest.shape[1])),XTest,axis=0)\n",
    "yHat = PredictLabel(XTest_aug,theta)\n",
    "\n",
    "# Confusion matrix and performance metrics.\n",
    "confusionMatrix = GetConfusionMatrix(yTest,yHat)\n",
    "print(confusionMatrix)\n",
    "\n",
    "# Performance metrics\n",
    "ComputeMetrics(confusionMatrix)\n",
    "\n",
    "# Error rate of logistic regression classifier\n",
    "errCount = confusionMatrix[1,0]+confusionMatrix[0,1]\n",
    "errRate = 100*errCount/len(yTest) # Percentage over total number of training data points\n",
    "print(\"Error of logistic regression: %2.2f%%.\"%errRate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<span style=\"color:black\">\n",
    "  \n",
    "What are your observations/conclusions based on the results?\n",
    "    \n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
